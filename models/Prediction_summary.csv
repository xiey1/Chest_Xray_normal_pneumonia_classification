Model,Accuracy,Normal_acc,Pneumonia_acc,True_pos,True_neg,False_pos,False_neg,Precision,Recall,F1_score,ROC_AUC,PRC_AUC,TotalCNN_scratch,0.751602564,0.354700855,0.98974359,386,83,151,4,0.718808194,0.98974359,0.832793959,0.915,0.945,624CNN_scratch_w,0.806089744,0.512820513,0.982051282,383,120,114,7,0.770623742,0.982051282,0.863585118,0.922,0.948,624Inception_v3,0.838141026,0.670940171,0.938461538,366,157,77,24,0.826185102,0.938461538,0.878751501,0.912,0.95,624Inception_v3_w,0.83974359,0.854700855,0.830769231,324,200,34,66,0.905027933,0.830769231,0.86631016,0.881,0.942,624Inception_v3_fc2,0.876602564,0.769230769,0.941025641,367,180,54,23,0.871733967,0.941025641,0.905055487,0.929,0.96,624Inception_v3_fc2_w,0.866987179,0.824786325,0.892307692,348,193,41,42,0.894601542,0.892307692,0.893453145,0.911,0.955,624